{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrtcuX1Gdy4jfRIVUjHHTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/curimm/JavaScript/blob/main/202306_08)DataCampus_PJ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYJEvBqvRc12"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "기사의 주제 찾기 및 내년의 핫토픽 예측(회귀)\n",
        "\n",
        "전처리 : 2000년부터 2018년까지 자료(2000~2010)\n",
        "\n",
        "웹크롤링(영어논문, Abstract)>엑셀>전처리\n",
        "논문의 문서별 단어토큰화:['토픽', '멋짐']\n",
        "정규표현식:문자열에 대해 원하는 검색패턴 지정\n",
        "불용어처리:추가로 작성하여 몇개의 단어를 추가로 제거\n",
        "3. 카운트 벡터화(doc2bow) : Gensim의 명령어\n",
        "예) 특성집합=['one', 'two', 'three', 'four', 'five']\n",
        "예) 주어진 문서의 토큰화 결과=['two', 'two', 'four']\n",
        "예) 변환된 카운트벡터 결과=[0, 2, 0, 1, 0]\n",
        "4. 토픽모델링(Gensim의 LdaModel사용)\n",
        "5. 혼란도와 응집도를 이용하여 토픽 개수의 최적값 선택\n",
        "6. 최적의 토픽값을 이용하여 토픽을 7개로 결정\n",
        "7. 토픽별 최빈 단어 10개를 활용하여 주제 찾기\n",
        "8. 선형회귀(X:year, Y:Topic 7개의 해당연도별 비중)를 이용하여 내년의 핫토픽을 예측\n"
      ],
      "metadata": {
        "id": "O2U14Tw8Rf-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import requests\n",
        "import csv\n",
        "import lxml                                     # API로 Abstract 자료를 받은 부분이 XML 자료임.\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "wnpfJJdvRg_q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfa = pd.read_excel(\"c:\\workspace\\environment.xls\")   # 65535 row 이상은 불러들이지 못함\n",
        "dfa\n",
        "\n",
        "\n",
        "df=dfa[['Year', 'Content']]     # 분석의 대상이 되는 \"연도(year)\"에 따른 \"Abstract\"를 DataFrame 구조로 읽어옴\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "eWckC_XFRtXw",
        "outputId": "e5b9ec43-f352-4d31-feda-61fbc9d198cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'c:\\\\workspace\\\\environment.xls'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bf95fe93ddd0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"c:\\workspace\\environment.xls\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 65535 row 이상은 불러들이지 못함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdfa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m     \u001b[0;31m# 분석의 대상이 되는 \"연도(year)\"에 따른 \"Abstract\"를 DataFrame 구조로 읽어옴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1494\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1497\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\workspace\\\\environment.xls'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = df.dropna()                             # Abstract의 결측치가 있어서 이를 제거함\n",
        "df"
      ],
      "metadata": {
        "id": "nAFYIvVMRx_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "euO4qavrR07g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PZTIwHEeR2EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.Content[0]                                            #첫번째 Abstract를 제대로 읽어왔는지 확인함\n"
      ],
      "metadata": {
        "id": "xKg0rYraR2qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.Content[1]                                            #두번째 Abstract를 제대로 읽어왔는지 확인함\n",
        "\n"
      ],
      "metadata": {
        "id": "1AtY4I2hR2oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In[70]:\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")     # nltk에서 제공한 불용어 사용\n",
        "RegTok = RegexpTokenizer(\"[\\w']{2,}\")            # 정규표현식으로 토크나이저를 정의. 빈칸없는 단어로 3글자 이상인 단어만 토큰화시킴\n",
        "english_stops = set(stopwords.words('english'))  # 금지어가 반복되지 않게 set으로 변환\n",
        "my_stopword=['\\r','대한','위해', '환경부', '2016', '는', '등을', '따라', '위한', '대해', '밝혔다', '했다',\n",
        "            '경우', '한다', '2013년', '2012년', '또한', '관련', '이번', '통해', '관련', '있는', '것으로',\n",
        "             '나타났다', '09', '가장', '지난', '지역', '기자', '함께', '예정이다', '있도록', '무단전재',\n",
        "            'kr', 'co', '받을', '변형', '기사에', '알려드립니다', '금합니다', '무단배포를', '때문에', '것이',\n",
        "            '말했다', '것이다', '있어', '많은', '것은', '아니라', '계획이다', '사업을', '2015년', '연간',\n",
        "            'ecolaw', '환경법률신문', '서울시', '서울시는', '처음', '으로', '20일', '따르면', '향으로', '각종',\n",
        "            '드는', '1백', '결정했다', '인천지방해양수산청에', '해양수산부는', '이번에', '따른','된다', '전례없이',\n",
        "            'www', 'http', '2011', '있으며', '다양한', '행사는', '쉽게', '환경일보', '하는', '같은', '에너지데일리',\n",
        "            '이를', '있으며', '기술', '08', '발생하는', '저작권은', '있음을', '이를', '있으며', '에코저널', '2017',\n",
        "            '에코저널에', '환경부는', '2012', '장관', '대상으로', '나갈', '올해', '환경일보', '결과', '평균', '비해',\n",
        "            '전국', '높은', '10', '12', '이상', '이에', '현재', '그러나', '인한', '있다고', '인해', '최근', '이라고',\n",
        "            '사업', '관계자는', '07', '사업은', '이날', '오는', '주제로', '대상으로', '오후', '가운데', '한편',\n",
        "            '2015', '2013', '향후', '환경', '조사', '특히', '전체', '각각', '11', '대비', '홈페이지', '분야', '에서',\n",
        "            '직접', '2014년', '저작권자', 'webmaster', '다른', '이러한', '있을', '있고', '매우', '것을', '새로',\n",
        "            'ecojournal', '등의', '또는', '환경법률신문에', 'kr이', '의견을', '진행된다', '2018', '방안을', '에코타임스',\n",
        "            'ecotiger', '2016년', '재배포금지', '저작권자', '피해를', '피해', '베트남', '라고', '있다는', '하지만',\n",
        "            '없는', '한다고', '교수는', '강조했다', '위해서는', '않고', '2009년', '인증을', '진행항다', '공유하고',\n",
        "            '무단', '최우수', '기존', '시는', '내년', '지난해', '이후', '모두', '이는', '발생한', '보다', '등에',\n",
        "             '에코타임스에', '대해서는', '해당', '통한', '추진할', '거쳐', 'daum', 'net', '만들기', '좋은',\n",
        "            '시민들의', '10월', '기술을', '시민들이', '우리', '명이', '등이', '아름다운', '동안', '제품', '2014',\n",
        "             '개발', '어길', '이정성', 'jungsung|', '인근', '진행됐다', '기준', '영향을', '확인됐다', '30',\n",
        "             '20', '증가한', '2017년', '2010', '하고', '않은', '일부', '않는', '문제가', '것이라고', '모든', '관한',\n",
        "            '주요', '실시하고', '경기도는', '적극', '일자리', '정부는', '평가를', '이용해', '기반', '많이', '원장',\n",
        "            '자세한', '연구를', '의원은', '그는', '라며', '지적했다', '필요하다', '한다는', '있지만', '설명했다',\n",
        "            '주장했다', '발생', '주변', '기대된다', '방침이다', '물론', '지속적으로', '개최한다', '지속가능한',\n",
        "            '이어', '개최했다', '연구', '기준을', '결과를', '낮은', '실시한', '절약', '국립공원', '피해가', '없다',\n",
        "            '만큼', '있었다', '때문이다', '이미', '않아', '대응', '방안', '당부했다', '만들어', '새로운', '지원한다',\n",
        "            '건강한', '전남',' 제주', '강원도', '추진', '있게', '필요한', '운영', '기여할', '세계', '국내', '글로벌',\n",
        "            '앞으로도', '어린이', '실시한다', '관내', '서울', '11월', '참석한', '9월', '계기가', '경기도', '추진하고',\n",
        "            '원을', '선정됐다', '수돗물을', '차량', '15', '태풍', '문제를', '제대로', '해야', '과정에서', '우리나라',\n",
        "            '함께하는', '금번', '중요한', '이용한', '사용', '정보를', '서비스를']\n",
        "# Abstract의 앞부분에 집중적으로 반복되는 주제와 상관없는 단어(연구의 목적)를\n",
        " ## 개인적으로 금지어로 정함\n",
        "\n",
        "def tokenizer(text):\n",
        "    tokens = RegTok.tokenize(text.lower())\n",
        "    words = [word for word in tokens if (word not in english_stops) and len(word)>1 and (word not in my_stopword)]\n",
        "    return words\n",
        "\n",
        "texts = [tokenizer(news) for news in df.Content]    # 토큰화가 잘 되어 있는 것을 확인함.\n",
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9nvZW3TdR2ln",
        "outputId": "d10418be-3ae3-4466-ebfa-37567421832c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-14bb98845e71>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# In[70]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcachedStopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# nltk에서 제공한 불용어 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mRegTok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\\w']{2,}\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# 정규표현식으로 토크나이저를 정의. 빈칸없는 단어로 3글자 이상인 단어만 토큰화시킴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menglish_stops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 금지어가 반복되지 않게 set으로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Gensim은 토큰화 결과로부터 토큰과 gensim 모듈이 내부적으로 사용하는 id를 매칭하는 사전을 만든다.\n",
        "# 이렇게 사전을 만들게 하는 클래스가 Dictionary이다. (사전생성)\n",
        "# filter_extremes() 메서드 : 빈도수가 높은 순으로 지정된 수의 단어를 특성으로 선택한다.\n",
        "# no_below=5 : 노출된 단어 수가 5번 미만의 단어는 버린다.\n",
        "# no_above=0.5 : 노출된 단어 비중이 50%를 넘는 단어는 버린다.\n",
        "\n",
        "# In[71]:\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary        #  gensim을 써서 카운트 벡터로 변환하고, 혼란도와 응집도를 구해서\n",
        "                                                         ## 토픽의 수를 결정하는데 사용함\n",
        "dictionary = Dictionary(texts)                           # 토큰화 결과로부터 dictionay 생성\n",
        "print('#Number of initial unique words in documents:', len(dictionary))\n",
        "\n",
        "                     # 단어 빈도수가 너무 적거나(5번 미만) 높은 단어(50%를 초과)를 배제하고 특성(단어)을 단어의 빈도 순으로 나타냄.\n",
        "dictionary.filter_extremes(keep_n=2000, no_below=5, no_above=0.5)     # 단어의 최대수를 2000으로 한정함\n",
        "print('#Number of unique words after removing rare and common words:', len(dictionary))\n",
        "\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]       # doc2bow(text) : 카운트 벡터로 변환\n",
        "print('#Number of unique tokens: %d' % len(dictionary))\n",
        "print('#Number of documents: %d' % len(corpus))\n"
      ],
      "metadata": {
        "id": "llVZMQF5R2jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "num_topics = 10                 #Topic수는 임의로 10으로 정한 후 혼란도와 응집도를 계산하는데 사용함.\n",
        "passes = 5                      # passes : 최대로 반복가능한 알고리즘수\n",
        "get_ipython().run_line_magic('time', 'model = LdaModel(corpus=corpus, id2word=dictionary,                        passes=passes, num_topics=num_topics,                         random_state=7)')\n"
      ],
      "metadata": {
        "id": "cYxyHZ5OR2gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print_topics() 메서드 : gensim 제공, 각 토픽의 상위비중단어(비중도 표시함)를 나타냄\n",
        "# num_words : 각 토픽의 상위 단어 수 지정\n",
        "\n",
        "# #  코딩 및 결과 도출, 불용어 처리중"
      ],
      "metadata": {
        "id": "dsYkScBmR2eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_topics(num_words=10)    # 결과에 의거 불용어 선정 피드백"
      ],
      "metadata": {
        "id": "fonU_C9VR2b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "UT7eRUYGR2VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feed thd LDA model in the pyLDAvis instance\n",
        "lda_viz = gensimvis.prepare(model, corpus, dictionary)\n",
        "lda_viz\n"
      ],
      "metadata": {
        "id": "FpTVJuyjSWsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "\n",
        "cm = CoherenceModel(model=model, corpus=corpus, coherence='u_mass')\n",
        "coherence = cm.get_coherence()\n",
        "print(coherence)"
      ],
      "metadata": {
        "id": "qUVarVdKSWp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def show_coherence(corpus, dictionary, start=3, end=15):\n",
        "    iter_num = []\n",
        "    per_value = []\n",
        "    coh_value = []\n",
        "\n",
        "    for i in range(start, end + 1):   # range(3, 16) : 3,4,...,15\n",
        "        model = LdaModel(corpus=corpus, id2word=dictionary,\n",
        "                 chunksize=1000, num_topics=i,\n",
        "                 random_state=7)\n",
        "        iter_num.append(i)\n",
        "        pv = model.log_perplexity(corpus)\n",
        "        per_value.append(pv)\n",
        "\n",
        "        cm = CoherenceModel(model=model, corpus=corpus,\n",
        "                            coherence='u_mass')\n",
        "        cv = cm.get_coherence()\n",
        "        coh_value.append(cv)\n",
        "        print(f'num_topics: {i}, perplexity: {pv:0.3f}, coherence: {cv:0.3f}')\n",
        "\n",
        "    plt.plot(iter_num, per_value, 'g-')\n",
        "    plt.xlabel(\"num_topics\")\n",
        "    plt.ylabel(\"perplexity\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(iter_num, coh_value, 'r--')\n",
        "    plt.xlabel(\"num_topics\")\n",
        "    plt.ylabel(\"coherence\")\n",
        "    plt.show()\n",
        "\n",
        "show_coherence(corpus, dictionary, start=3, end=15)\n"
      ],
      "metadata": {
        "id": "URS_8odfSWnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 혼란도가 낮으면서 응집도가 높은 것을 4로 선택함.\n",
        "## 사이킷런을 이용한 토픽모델링\n",
        "from sklearn.feature_extraction.text import CountVectorizer  #LDA는 카운트벡터를 입력으로 사용한다. 카운트벡터 생성과정 중 토큰화과정.\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "RegTok = RegexpTokenizer(\"[\\w']{2,}\")                        # 정규표현식으로 토크나이저를 정의\n",
        "english_stops = set(stopwords.words('english'))              # 금지어를 반복되지 않게 set으로 변환\n",
        "my_stopword=['\\r','대한','위해', '환경부', '2016', '는', '등을', '따라', '위한', '대해', '밝혔다', '했다',\n",
        "            '경우', '한다', '2013년', '2012년', '또한', '관련', '이번', '통해', '관련', '있는', '것으로',\n",
        "             '나타났다', '09', '가장', '지난', '지역', '기자', '함께', '예정이다', '있도록', '무단전재',\n",
        "            'kr', 'co', '받을', '변형', '기사에', '알려드립니다', '금합니다', '무단배포를', '때문에', '것이',\n",
        "            '말했다', '것이다', '있어', '많은', '것은', '아니라', '계획이다', '사업을', '2015년', '연간',\n",
        "            'ecolaw', '환경법률신문', '서울시', '서울시는', '처음', '으로', '20일', '따르면', '향으로', '각종',\n",
        "            '드는', '1백', '결정했다', '인천지방해양수산청에', '해양수산부는', '이번에', '따른','된다', '전례없이',\n",
        "            'www', 'http', '2011', '있으며', '다양한', '행사는', '쉽게', '환경일보', '하는', '같은', '에너지데일리',\n",
        "            '이를', '있으며', '기술', '08', '발생하는', '저작권은', '있음을', '이를', '있으며', '에코저널', '2017',\n",
        "            '에코저널에', '환경부는', '2012', '장관', '대상으로', '나갈', '올해', '환경일보', '결과', '평균', '비해',\n",
        "            '전국', '높은', '10', '12', '이상', '이에', '현재', '그러나', '인한', '있다고', '인해', '최근', '이라고',\n",
        "            '사업', '관계자는', '07', '사업은', '이날', '오는', '주제로', '대상으로', '오후', '가운데', '한편',\n",
        "            '2015', '2013', '향후', '환경', '조사', '특히', '전체', '각각', '11', '대비', '홈페이지', '분야', '에서',\n",
        "            '직접', '2014년', '저작권자', 'webmaster', '다른', '이러한', '있을', '있고', '매우', '것을', '새로',\n",
        "            'ecojournal', '등의', '또는', '환경법률신문에', 'kr이', '의견을', '진행된다', '2018', '방안을', '에코타임스',\n",
        "            'ecotiger', '2016년', '재배포금지', '저작권자', '피해를', '피해', '베트남', '라고', '있다는', '하지만',\n",
        "            '없는', '한다고', '교수는', '강조했다', '위해서는', '않고', '2009년', '인증을', '진행항다', '공유하고',\n",
        "            '무단', '최우수', '기존', '시는', '내년', '지난해', '이후', '모두', '이는', '발생한', '보다', '등에',\n",
        "             '에코타임스에', '대해서는', '해당', '통한', '추진할', '거쳐', 'daum', 'net', '만들기', '좋은',\n",
        "            '시민들의', '10월', '기술을', '시민들이', '우리', '명이', '등이', '아름다운', '동안', '제품', '2014',\n",
        "             '개발', '어길', '이정성', 'jungsung|', '인근', '진행됐다', '기준', '영향을', '확인됐다', '30',\n",
        "             '20', '증가한', '2017년', '2010', '하고', '않은', '일부', '않는', '문제가', '것이라고', '모든', '관한',\n",
        "            '주요', '실시하고', '경기도는', '적극', '일자리', '정부는', '평가를', '이용해', '기반', '많이', '원장',\n",
        "            '자세한', '연구를', '의원은', '그는', '라며', '지적했다', '필요하다', '한다는', '있지만', '설명했다',\n",
        "            '주장했다', '발생', '주변', '기대된다', '방침이다', '물론', '지속적으로', '개최한다', '지속가능한',\n",
        "            '이어', '개최했다', '연구', '기준을', '결과를', '낮은', '실시한', '절약', '국립공원', '피해가', '없다',\n",
        "            '만큼', '있었다', '때문이다', '이미', '않아', '대응', '방안', '당부했다', '만들어', '새로운', '지원한다',\n",
        "            '건강한', '전남',' 제주', '강원도', '추진', '있게', '필요한', '운영', '기여할', '세계', '국내', '글로벌',\n",
        "            '앞으로도', '어린이', '실시한다', '관내', '서울', '11월', '참석한', '9월', '계기가', '경기도', '추진하고',\n",
        "            '원을', '선정됐다', '수돗물을', '차량', '15', '태풍', '문제를', '제대로', '해야', '과정에서', '우리나라',\n",
        "            '함께하는', '금번', '중요한', '이용한', '사용', '정보를', '서비스를']\n",
        "\n",
        "def tokenizer(text):\n",
        "    tokens = RegTok.tokenize(text.lower())\n",
        "    words = [word for word in tokens if (word not in english_stops) and len(word)>1 and (word not in my_stopword)]\n",
        "    return words\n",
        "\n",
        "texts = [tokenizer(news) for news in df.Content]          # 토큰화 완료\n",
        "texts"
      ],
      "metadata": {
        "id": "AvzlMfEhSWi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # LDA는 카운트벡터를 입력으로 사용한다.\n",
        "\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "RegTok = RegexpTokenizer(\"[\\w']{2,}\")                         # 정규표현식으로 토크나이저를 정의\n",
        "english_stops = set(stopwords.words('english'))               # 반복되지 않게 set으로 변환\n",
        "my_stopword=['\\r','대한','위해', '환경부', '2016', '는', '등을', '따라', '위한', '대해', '밝혔다', '했다',\n",
        "            '경우', '한다', '2013년', '2012년', '또한', '관련', '이번', '통해', '관련', '있는', '것으로',\n",
        "             '나타났다', '09', '가장', '지난', '지역', '기자', '함께', '예정이다', '있도록', '무단전재',\n",
        "            'kr', 'co', '받을', '변형', '기사에', '알려드립니다', '금합니다', '무단배포를', '때문에', '것이',\n",
        "            '말했다', '것이다', '있어', '많은', '것은', '아니라', '계획이다', '사업을', '2015년', '연간',\n",
        "            'ecolaw', '환경법률신문', '서울시', '서울시는', '처음', '으로', '20일', '따르면', '향으로', '각종',\n",
        "            '드는', '1백', '결정했다', '인천지방해양수산청에', '해양수산부는', '이번에', '따른','된다', '전례없이',\n",
        "            'www', 'http', '2011', '있으며', '다양한', '행사는', '쉽게', '환경일보', '하는', '같은', '에너지데일리',\n",
        "            '이를', '있으며', '기술', '08', '발생하는', '저작권은', '있음을', '이를', '있으며', '에코저널', '2017',\n",
        "            '에코저널에', '환경부는', '2012', '장관', '대상으로', '나갈', '올해', '환경일보', '결과', '평균', '비해',\n",
        "            '전국', '높은', '10', '12', '이상', '이에', '현재', '그러나', '인한', '있다고', '인해', '최근', '이라고',\n",
        "            '사업', '관계자는', '07', '사업은', '이날', '오는', '주제로', '대상으로', '오후', '가운데', '한편',\n",
        "            '2015', '2013', '향후', '환경', '조사', '특히', '전체', '각각', '11', '대비', '홈페이지', '분야', '에서',\n",
        "            '직접', '2014년', '저작권자', 'webmaster', '다른', '이러한', '있을', '있고', '매우', '것을', '새로',\n",
        "            'ecojournal', '등의', '또는', '환경법률신문에', 'kr이', '의견을', '진행된다', '2018', '방안을', '에코타임스',\n",
        "            'ecotiger', '2016년', '재배포금지', '저작권자', '피해를', '피해', '베트남', '라고', '있다는', '하지만',\n",
        "            '없는', '한다고', '교수는', '강조했다', '위해서는', '않고', '2009년', '인증을', '진행항다', '공유하고',\n",
        "            '무단', '최우수', '기존', '시는', '내년', '지난해', '이후', '모두', '이는', '발생한', '보다', '등에',\n",
        "             '에코타임스에', '대해서는', '해당', '통한', '추진할', '거쳐', 'daum', 'net', '만들기', '좋은',\n",
        "            '시민들의', '10월', '기술을', '시민들이', '우리', '명이', '등이', '아름다운', '동안', '제품', '2014',\n",
        "             '개발', '어길', '이정성', 'jungsung|', '인근', '진행됐다', '기준', '영향을', '확인됐다', '30',\n",
        "             '20', '증가한', '2017년', '2010', '하고', '않은', '일부', '않는', '문제가', '것이라고', '모든', '관한',\n",
        "            '주요', '실시하고', '경기도는', '적극', '일자리', '정부는', '평가를', '이용해', '기반', '많이', '원장',\n",
        "            '자세한', '연구를', '의원은', '그는', '라며', '지적했다', '필요하다', '한다는', '있지만', '설명했다',\n",
        "            '주장했다', '발생', '주변', '기대된다', '방침이다', '물론', '지속적으로', '개최한다', '지속가능한',\n",
        "            '이어', '개최했다', '연구', '기준을', '결과를', '낮은', '실시한', '절약', '국립공원', '피해가', '없다',\n",
        "            '만큼', '있었다', '때문이다', '이미', '않아', '대응', '방안', '당부했다', '만들어', '새로운', '지원한다',\n",
        "            '건강한', '전남',' 제주', '강원도', '추진', '있게', '필요한', '운영', '기여할', '세계', '국내', '글로벌',\n",
        "            '앞으로도', '어린이', '실시한다', '관내', '서울', '11월', '참석한', '9월', '계기가', '경기도', '추진하고',\n",
        "            '원을', '선정됐다', '수돗물을', '차량', '15', '태풍', '문제를', '제대로', '해야', '과정에서', '우리나라',\n",
        "            '함께하는', '금번', '중요한', '이용한', '사용', '정보를', '서비스를']\n",
        "\n",
        "def tokenizer(text):\n",
        "    tokens = RegTok.tokenize(text.lower())\n",
        "    words = [word for word in tokens if (word not in english_stops) and len(word)>1 and (word not in my_stopword)]\n",
        "    return words\n",
        "\n",
        "vec = CountVectorizer(tokenizer=tokenizer,\n",
        "                      max_df=0.5, min_df = 5,\n",
        "                      max_features = 2000)                   # 최대 사용 단어수를 2000으로 선택\n",
        "\n",
        "child_cv = vec.fit_transform(df.Content)                    # 카운트벡터화 완료함. 단어수가 1627임.\n",
        "print(child_cv.shape)\n"
      ],
      "metadata": {
        "id": "8flqzPcYSdcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components = 4,                       # n_components = 4 : 토픽의 수를 4로 고정함\n",
        "                                n_jobs= -1,\n",
        "                                random_state=0)\n",
        "\n",
        "get_ipython().run_line_magic('time', 'child_topics = lda.fit_transform(child_cv)')\n",
        "\n",
        "def print_top_words(model, feature_names, n_top_words):                # n_top_words=10 : 각각의 토픽에 빈도수가 높은 순으로 단어 10개를 나타냄\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        # enumerate() : 매개변수에 리스트를 넣으면 인텍스와 값을 쌍으로 사용해 반복문을 돌릴 수 있게 해주는 함수\n",
        "        print(\"Topic #%d: \" % topic_idx, end='')\n",
        "        print(\", \".join([feature_names[i]\n",
        "                         for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "                        # topic.argsort()[:-n_top_words - 1:-1] : argsort() 할 때 내림차순으로 정열해라.\n",
        "print_top_words(lda,vec.get_feature_names(), 10)            # 토픽별 최빈단어순으로 문자를 나타냄.\n"
      ],
      "metadata": {
        "id": "E218J0h6SgTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trend_data = pd.DataFrame(child_topics, columns=['Topic'+str(i) for i in range(1, 5)])\n",
        "trend_data\n"
      ],
      "metadata": {
        "id": "eBe-o7MlSiJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trend_data = pd.concat([trend_data, df.Year], axis=1)\n",
        "trend_data\n"
      ],
      "metadata": {
        "id": "LY7o0BEASkNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trend_data = trend_data.dropna()   # 결측치가 있는 데이터는 행기준으로 통째로 없애라\n",
        "trend_data\n",
        "\n"
      ],
      "metadata": {
        "id": "a5hVrJZfSoYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trend = trend_data.groupby(['Year']).mean()\n",
        "trend.iloc[:, :]  # 모든 행의 인덱스, 모든 열의 인덱스에 해당하는 데이터를 모두 불러와라.\n"
      ],
      "metadata": {
        "id": "m88kr7Z8SoV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, sharex='col', figsize=(12, 16))\n",
        "\n",
        "  # figsize=(12, 16) : 가로 12인치, 세로 16인치의 도화지에\n",
        "  # 2, 2 : 도화지를 가로로 2등분, 세로로 2등분.\n",
        "  # 각각의 영역에 플롯을 하나씩 배치하여 총 4개의 그래프를 그림\n",
        "  # sharex='col' : 같은 컬럼에 위치한 그래프는 x축을 공유함\n",
        "\n",
        "for col, ax in zip(trend.columns.tolist(), axes.ravel()):\n",
        "    # trend.columns.tolist() : trend의 칼럼들 numpy array -> list\n",
        "    # axes.ravel() : axes의 리스트를 1차원으로 배열시킴\n",
        "    ax.set_title(col)  # 서브플롯 제목을 나타내라\n",
        "    ax.axes.xaxis.set_visible(True)   # 서브플롯 x축 간격을 보이도록 나타내라\n",
        "    ax.plot(trend[col])   # trend[col]의 플롯들을 그려라\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nf4AQDGDSoTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "type(trend)"
      ],
      "metadata": {
        "id": "5kYMToROSoL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trend.reset_index(inplace=True)\n",
        "trend\n"
      ],
      "metadata": {
        "id": "s_GvJ3lISoER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "for num in range(4):\n",
        "    data = trend[['Year']]\n",
        "    label = trend['Topic{}'.format(num+1)]\n",
        "    model = LinearRegression()                          # 선형회귀에 적용함. X(data)는 연도, Y(label)은 해당연도의 Topic의 비중임.\n",
        "    model = model.fit(data, label)\n",
        "    predict = model.predict([[2019.0]])     # model.predict([[2023.0]])\n",
        "    print(\"2019, Topic{} : \".format(num+1), predict)    # 2023년에 가장 핫한 토픽은 Topic4(스스로학습법과 육아의 효능관계)로 예상함.   Main 발표 끝 -\n"
      ],
      "metadata": {
        "id": "rufCG6G-SoAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def power(x):\n",
        "    return x * x\n",
        "a = power(2)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "Mr1Kv8CSTh34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def power(x):\n",
        "    return x * x\n",
        "a = map(power, [1,2,3])  # map함수는 리스트의 요소를 함수에 넣고 리턴된 값으로 새로운 리스트를 구성하는 함수\n",
        "print(list(a))"
      ],
      "metadata": {
        "id": "8MPaMeIfTjRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "b = map(lambda x: x * x, [1,2,3])\n",
        "# lambda는 함수를 def로 정의(함수이름 필요함)하지 않고 간단하게 함수(함수이름 필요없음)를 정의하는 것\n",
        "print(list(b))\n"
      ],
      "metadata": {
        "id": "fJ0cbGMVTkw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 람다 함수\n",
        "# x = 2023-07-31\n",
        "a=map(lambda x: x[:4], [\"2023-07-31\"])\n",
        "print(list(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wS-OB6sTl-m",
        "outputId": "810c1da8-df2d-47e2-f1ea-71bfac9ed86e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2023']\n"
          ]
        }
      ]
    }
  ]
}